{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 1.1. To identify missing values, you can use the isna() or isnull() methods, both of which will return a DataFrame or Series of the same shape with True for missing values and False for non-missing values. To handle missing values, we can drop rows or columns containing missing values using the dropna() method or impute or fill missing values using \n",
    "the fillna() method.\n",
    "\n",
    "1.2. Imputation is the process of replacing missing values with estimated ones based on the data. It can be useful because removing rows with missing values may lead to loss of important information, especially if the missing values are in many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exam_data = {\n",
    "    'name': ['Anastasia', 'Dima', 'Katherine', 'James', None],  \n",
    "    'score': [12.5, 9, 16.5, np.nan, np.nan],  \n",
    "    'attempts': [1, 3, 2, 3, 2],  \n",
    "    'qualify': [None, 'no', 'yes', 'no', 'no']  \n",
    "}\n",
    "\n",
    "labels = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "df = pd.DataFrame(exam_data, index=labels)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nIdentifying missing values:\")\n",
    "print(df.isna())\n",
    "\n",
    "df['score'] = df['score'].fillna(df['score'].mean())\n",
    "df['qualify'] = df['qualify'].fillna(df['qualify'].mode()[0])\n",
    "df['name'] = df['name'].fillna('Unknown')\n",
    "\n",
    "print(\"\\nDataFrame after Imputation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 2.1. Categorical variables are non-numeric variables that take on a limited, fixed number of values. There are several ways to encode categorical variables in a Pandas DataFrame:\n",
    "Label Encoding: Each category is mapped to a unique integer.\n",
    "One-Hot Encoding: Each unique category value is converted into a new binary (0 or 1) column, representing whether that category is present or not.\n",
    "\n",
    "2.2. One-hot encoding is a technique used to convert categorical variables into binary vectors. Each category in the original variable is turned into a separate binary column where each row will have a 1 for the category that corresponds to that row and 0 for all other categories. It's most useful when dealing with nominal categorical variables (no intrinsic order) and when preparing data for algorithms that require numeric input, such as linear regression, logistic regression, or neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'John'],\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "df['color_encoded'] = pd.factorize(df['color'])[0]\n",
    "print(\"\\nDataFrame with Label Encoding:\")\n",
    "print(df)\n",
    "\n",
    "df_one_hot = pd.get_dummies(df, columns=['color'], prefix=['color'])\n",
    "print(\"\\nDataFrame with One-Hot Encoding:\")\n",
    "print(df_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 3.1. In Pandas, duplicate rows can be identified using duplicated() and removed using drop_duplicates(). \n",
    "\n",
    " 3.2. duplicated() - identifies duplicate rows but does not remove them; returns a boolean Series where True indicates that a row is a duplicate of a previous row.\n",
    " drop_duplicates() - removes duplicate rows from the DataFrame; by default, it keeps the first occurrence and removes the subsequent duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'James', 'Dima'],\n",
    "    'score': [12.5, 9, 16.5, 10, 10, 9],\n",
    "    'attempts': [1, 3, 2, 3, 3, 3]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nIdentifying duplicate rows using duplicated():\")\n",
    "print(df.duplicated())  \n",
    "\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "print(\"\\nDataFrame after removing duplicates with drop_duplicates():\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 4.1. Feature scaling plays a crucial role in machine learning by ensuring that all features are on a similar scale, preventing bias towards certain features. It improves the performance and convergence of many machine learning algorithms.\n",
    "\n",
    "4.2. Min-max scaling, also known as normalization, rescales values to a fixed range, usually [0,1] (or sometimes [-1,1]). Formula: subtract x(min) from x and divide by x(max) - x(min). The smallest value becomes 0, the largest becomes 1, and other values are proportionally scaled. It normalizes data so that all features have the same scale.\n",
    "It helps machine learning models work better. It preserves relationships between values. Retains the original shape of the data. Use Min-Max Scaling when: the data is already within a fixed range; the algorithm does not assume a normal distribution; it is needed to preserve the relationship between original values.\n",
    "\n",
    "Z-score normalization, also known as standardization, transforms data so that it has a mean of 0 and a standard deviation of 1. Formula: subract mean from x and divide by standard deviation. Centers data around 0, making it easier to compare different distributions. Changes distribution shape. Use Z-Score Normalization when: the data has different units or scales; the algorithm assumes normal distribution; there are extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'age': [22, 25, 30, 35, 40],\n",
    "    'salary': [30000, 45000, 50000, 65000, 70000],\n",
    "    'score': [88, 92, 85, 90, 87]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Min-Max Scaling:\n",
    "def min_max_scaling(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "df_min_max_scaled = min_max_scaling(df)\n",
    "\n",
    "print(\"\\nDataFrame after Min-Max Scaling:\")\n",
    "print(df_min_max_scaled)\n",
    "\n",
    "#Z-Score Normalization:\n",
    "def z_score_normalization(df):\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "df_z_score_scaled = z_score_normalization(df)\n",
    "\n",
    "print(\"\\nDataFrame after Z-Score Normalization:\")\n",
    "print(df_z_score_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) 5.1. Outliers are data points that significantly differ from the rest of the dataset. They may arise due to measurement errors, rare events, or natural variations in data.\n",
    "They distort statistics like the mean and standard deviation.\n",
    "They affect machine learning models that rely on assumptions about the data distribution (e.g., linear regression, k-NN).\n",
    "They increase model complexity and reduce accuracy.\n",
    "\n",
    "5.2. Methods to detect outliers:\n",
    "-Using the IQR (Interquartile Range) Method\n",
    "The IQR method considers values that lie outside 1.5 times the interquartile range as outliers.\n",
    "-Using Z-score (Standard Deviation Method)\n",
    "The Z-score method identifies values that are more than 3 standard deviations away from the mean.\n",
    "\n",
    "5.3. Handling Outliers in a Continuous Numerical Variable: \n",
    "-Removing Outliers (If outliers result from data entry errors, you can remove them)\n",
    "-Replacing Outliers (Instead of removing, replace outliers with min/max values within a threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({'Value': [10, 12, 100, 15, -50, 20]})\n",
    "\n",
    "print(\"Original DataFrame:\\n\", data)\n",
    "\n",
    "Q1 = data['Value'].quantile(0.25)\n",
    "Q3 = data['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data[(data['Value'] < lower_bound) | (data['Value'] > upper_bound)]\n",
    "print(\"\\nOutliers detected:\\n\", outliers)\n",
    "\n",
    "cleaned_data = data[(data['Value'] >= lower_bound) & (data['Value'] <= upper_bound)]\n",
    "print(\"\\nData after removing outliers:\\n\", cleaned_data)\n",
    "\n",
    "replaced_data = data.copy()\n",
    "replaced_data['Value'] = replaced_data['Value'].clip(lower=lower_bound, upper=upper_bound)\n",
    "print(\"\\nData with replaced outliers:\\n\", replaced_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
